{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Absa_50.03_Token Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4RZ6QHOPX2i"
      },
      "source": [
        "##**Absa 50.03: Token Classification**\n",
        "> Task (Pretrained model choice): BertForTokenClassification ('bert-base-chinese')\n",
        "\n",
        "> Dataset: (4/12 updated) 2243 labelled texts  \n",
        "\n",
        "> Splitting: Training: 1795 for training, 448 for validation. (w/o suffle)\n",
        "\n",
        "> Trainer source code: https://github.com/huggingface/transformers/tree/master/examples/token-classification\n",
        "\n",
        "> Model Performance:\n",
        "\n",
        "[詳盡的tag-separated performance](https://docs.google.com/document/d/16jQvs6bCiJw2848jnXnWamhZfcY_unCOJa3FuPm8E4Y/edit)\n",
        "\n",
        "    eval_overall_accuracy     =     0.7969\n",
        "    eval_overall_f1           =     0.7969\n",
        "\n",
        "\n",
        "\n",
        "> Notes:\n",
        "  *  csv file不能讀解法(使用jsonline files)：https://github.com/huggingface/transformers/issues/8698 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT6ZEhD1nvsn"
      },
      "source": [
        "##Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xDwQA9rgZaF"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import csv\n",
        "import json\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5HaFmQnzKa"
      },
      "source": [
        "## Import seq-pair files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agifoMYTgMT6",
        "outputId": "2b26c5db-134d-447b-a83e-7c940d2c1463"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVp-owaXgMzx"
      },
      "source": [
        "filename = '/content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/seq_pairs_20210412.pkl'\n",
        "with open(filename, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa1C2m01gV6i",
        "outputId": "80f7ce0a-97b9-462c-da9b-dd0760e14a45"
      },
      "source": [
        "# format: a list of tuples:(text, tags)\n",
        "print(f'Dataset size: {len(data)}')\n",
        "for pair in data[710:715]:\n",
        "  text = pair[0]\n",
        "  tag = pair[1]\n",
        "  for i in range(len(text)):\n",
        "    print(f'{text[i]}|{tag[i]}', end = ' , ')\n",
        "  print('\\n')\n",
        "print(data[0][0])\n",
        "print(data[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 2242\n",
            "中|B-E , 華|I-E , 5|B-A , 8|I-A , 8|I-A , 其|B-O , 實|B-O , 也|B-V , 還|I-V , 好|I-V , 不|I-V , 算|I-V , 多|I-V , 差|I-V , 啦|I-V , \n",
            "\n",
            "不|B-O , 划|B-O , 算|B-O , ，|B-O , 當|B-O , 初|B-O , 幫|B-O , 我|B-O , 弟|B-O , 算|B-O , 了|B-O , 一|B-O , 下|B-O , 現|B-O , 在|B-O , p|B-O , r|B-O , o|B-O , 耳|B-O , 機|B-O , 一|B-O , 直|B-O , 降|B-O , 價|B-O , ，|B-O , 還|B-O , 不|B-O , 如|B-O , 直|B-O , 辦|B-O , 4|B-A , 8|I-A , 8|I-A , 比|B-V , 較|I-V , 好|I-V , 除|B-O , 非|B-O , 你|B-O , 真|B-O , 的|B-O , 想|B-O , 用|B-O , 耳|B-O , 機|B-O , ！|B-O , \n",
            "\n",
            "市|B-A , 話|I-A , 送|I-A , 1|I-A , 6|I-A , 0|I-A , 分|I-A , 真|B-V , 的|I-V , 很|I-V , 划|I-V , 算|I-V , \n",
            "\n",
            "沒|B-O , 辦|B-O , 法|B-O , 生|B-O , 意|B-O , 做|B-O , 很|B-O , 大|B-O , 一|B-O , 定|B-O , 要|B-O , 打|B-O , 市|B-A , 話|I-A , 跟|I-A , 網|I-A , 外|I-A , X|B-O , D|B-O , 之|B-O , 前|B-O , 還|B-O , 有|B-O , 送|B-O , 簡|B-O , 訊|B-O , 還|B-O , 能|B-O , 簡|B-O , 訊|B-O , 檢|B-O , 舉|B-O , 違|B-O , 規|B-O , 停|B-O , 車|B-O , 超|B-V , 好|I-V , 用|I-V , 的|I-V , \n",
            "\n",
            "補|B-O , 充|B-O , ：|B-O , 台|B-E , 哥|I-E , 大|I-E , 在|B-O , 蘭|B-A , 潭|I-A , 也|B-O , 爛|B-V , 的|I-V , 跟|I-V , 渣|I-V , 一|I-V , 樣|I-V , \n",
            "\n",
            "台星的態度就是在等宿主台哥，逸以待勞，準備寄生。\n",
            "('台星的態度就是在等宿主台哥，逸以待勞，準備寄生。', ['B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA6fttBTn3qE"
      },
      "source": [
        "## Helper functions\n",
        "transform seq-pairs to the format for transformer trainer \n",
        "(.csv, .json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4c__o_lgWcW"
      },
      "source": [
        "def WriteCSV(data_fraction, output_path):\n",
        "  results = [['text', 'tags']]\n",
        "  texts = [pair[0] for pair in data_fraction]\n",
        "  tags = [pair[1] for pair in data_fraction]\n",
        "  for te, ta in zip(texts, tags):\n",
        "    results.append([te, ta])\n",
        "  with open(output_path, 'w', newline = '') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(results)\n",
        "  print(f'Finished writing into {output_path}.')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw-xtKMpK-Fd"
      },
      "source": [
        "PATH  = '/content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner'\n",
        "# Find the latest dir, get the seq_pairs, and build the bert_ner dir \n",
        "def FindDir(dir_path = '/content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup'): \n",
        "  dir_path = Path(dir_path)   \n",
        "  chdirs = sorted([x for x in dir_path.iterdir() if x.is_dir])\n",
        "  target_name = chdirs[-1].name\n",
        "  LAST_DIR_PATH =  dir_path+'/'+target_name+'/bert_ner'    \n",
        "  ! mkdir -p \"${LAST_DIR_PATH}\"\n",
        "  return LAST_DIR_PATH\n",
        "# PATH  = LAST_DIR_PATH"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I2QH7a_g_E3"
      },
      "source": [
        "### CSV FILES ###\n",
        "ratio = round(len(data)*0.8)\n",
        "train_csv_path = PATH + '/cht_ner_train.csv'\n",
        "val_csv_path = PATH + '/cht_ner_val.csv'\n",
        "# WriteCSV(data[:ratio], train_csv_path)\n",
        "# WriteCSV(data[ratio:], val_csv_path)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4aQPM7X9gA0",
        "outputId": "a39392f9-06b2-434c-e312-dbf97dbe5353"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1anh5MG6Qqu"
      },
      "source": [
        "import jsonlines\n",
        "def WriteJSONL(data_fraction, output_path):\n",
        "  '''\n",
        "  要求格式類型（下為一個json dict）\n",
        "  {\"text\": [\"B7 台哥大其他地方都很快，只有在地下室，超級爛！！！訊號會直接不見，我同學用中華和遠傳都還收得到\"],\n",
        "   \"tags\": [\"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"I-V\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\", \"B-O\"]}\n",
        "  一份json file裡包含多個如上格式的json dict\n",
        "   '''\n",
        "  texts = [pair[0] for pair in data_fraction]\n",
        "  tags = [pair[1] for pair in data_fraction]\n",
        "  objs = [{'text':[x], 'tags':y} for x,y in zip(texts, tags)]\n",
        "  with jsonlines.open(output_path, mode='w') as writer:\n",
        "    writer.write_all(objs)\n",
        "  # reading: json_object = json.loads(json_dump)\n",
        "  print(f'Finished writing into {output_path}.')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI44VH817lHq",
        "outputId": "57bea9d0-6d0f-4abf-e3bb-3eee6e306303"
      },
      "source": [
        "### JSON FILES ###\n",
        "ratio = round(len(data)*0.8)\n",
        "train_json_path = PATH + '/cht_ner_train.json'\n",
        "val_json_path = PATH + '/cht_ner_val.json'\n",
        "WriteJSONL(data[:ratio], train_json_path)\n",
        "WriteJSONL(data[ratio:], val_json_path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished writing into /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/cht_ner_train.json.\n",
            "Finished writing into /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/cht_ner_val.json.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5-rFQaFoE7h"
      },
      "source": [
        "## Install packages for trainer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-WGud2ek1H5",
        "outputId": "20cb46e1-b086-44d2-abe9-d17e405918a8"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOWaU8d1nIBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbb2609-36f0-4d8f-e29a-a9ef63a66adf"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQzRftOmk2ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28540415-26bf-4d78-a17f-e13fed38b8b9"
      },
      "source": [
        "# A source install\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-p36mo3tk\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-p36mo3tk\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.6.0.dev0 from git+https://github.com/huggingface/transformers in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.44)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2107872 sha256=f7bcb817ec8bb6a163daf511ccac6a38ec50ca51d08310a2424454703560607f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ciwqoi9b/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjzbSQZ0lM6A"
      },
      "source": [
        "# NO TRAINER VERSION\n",
        "\n",
        "# !python /content/transformers/examples/token-classification/run_ner_no_trainer.py \\\n",
        "#   --model_name_or_path bert-base-chinese \\\n",
        "#   --task_name ner \\\n",
        "#   --train_file /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/cht_ner_train.json\\\n",
        "#   --validation_file /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/cht_ner_val.json\\\n",
        "#   --max_length 128 \\\n",
        "#   --per_device_train_batch_size 32 \\\n",
        "#   --learning_rate 2e-5 \\\n",
        "#   --num_train_epochs 10 \\\n",
        "#   --output_dir /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/\\\n",
        "#   --seed 42069\\"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmNYQTWDlHH2"
      },
      "source": [
        "# NO TRAINER VERSION's instructions\n",
        "# !pip install accelerate\n",
        "# !accelerate config\n",
        "# !accelerate test "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFNkzWSV-I_T",
        "outputId": "c2d93ca9-753d-4278-c5b5-fcd46828f0bf"
      },
      "source": [
        "# TRAINER VERSION's eval metric\n",
        "!pip install seqeval "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkbtVjr-oPIA"
      },
      "source": [
        "##Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fumz79bToUuN",
        "outputId": "86ca9f62-445c-4bed-ea90-bda7e3190f18"
      },
      "source": [
        "# TRAINER VERSION\n",
        "!python /content/transformers/examples/token-classification/run_ner.py \\\n",
        "  --model_name bert-base-chinese \\\n",
        "  --task_name ner \\\n",
        "  --train_file /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/cht_ner_train.json\\\n",
        "  --validation_file /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/cht_ner_val.json\\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --output_dir /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out\\\n",
        "  --seed 42069 \\\n",
        "  --pad_to_max_length True\\\n",
        "  --return_entity_level_metrics True\\\n",
        "  --do_train \\\n",
        "  --do_eval \n",
        "  # --max_length 128 \\\n",
        "  # 42069"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-15 07:59:22.782694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/15/2021 07:59:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/15/2021 07:59:24 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr15_07-59-24_d682295f317c, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42069, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
            "04/15/2021 07:59:25 - WARNING - datasets.builder -   Using custom data configuration default-265f8df371d7124d\n",
            "04/15/2021 07:59:25 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-265f8df371d7124d/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
            "[INFO|configuration_utils.py:491] 2021-04-15 07:59:25,777 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
            "[INFO|configuration_utils.py:527] 2021-04-15 07:59:25,778 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-15 07:59:26,336 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
            "[INFO|configuration_utils.py:527] 2021-04-15 07:59:26,336 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-15 07:59:29,128 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-15 07:59:29,128 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-15 07:59:29,128 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-15 07:59:29,128 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-15 07:59:29,128 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|modeling_utils.py:1075] 2021-04-15 07:59:29,691 >> loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\n",
            "[WARNING|modeling_utils.py:1196] 2021-04-15 07:59:32,723 >> Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1207] 2021-04-15 07:59:32,723 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "04/15/2021 07:59:32 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-265f8df371d7124d/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-954f4fce5180f7f1.arrow\n",
            "04/15/2021 07:59:32 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-265f8df371d7124d/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-fa61ceb2af0ce03b.arrow\n",
            "[INFO|trainer.py:490] 2021-04-15 07:59:36,910 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: text, tags.\n",
            "[INFO|trainer.py:490] 2021-04-15 07:59:36,911 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: text, tags.\n",
            "[INFO|trainer.py:938] 2021-04-15 07:59:37,083 >> Loading model from /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/checkpoint-1000).\n",
            "[INFO|configuration_utils.py:489] 2021-04-15 07:59:37,086 >> loading configuration file /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:527] 2021-04-15 07:59:37,086 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-chinese\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1073] 2021-04-15 07:59:37,088 >> loading weights file /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1204] 2021-04-15 07:59:40,108 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:1213] 2021-04-15 07:59:40,108 >> All the weights of BertForTokenClassification were initialized from the model checkpoint at /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/checkpoint-1000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
            "[INFO|trainer.py:1030] 2021-04-15 07:59:41,591 >> ***** Running training *****\n",
            "[INFO|trainer.py:1031] 2021-04-15 07:59:41,591 >>   Num examples = 1794\n",
            "[INFO|trainer.py:1032] 2021-04-15 07:59:41,591 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1033] 2021-04-15 07:59:41,591 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1034] 2021-04-15 07:59:41,591 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1035] 2021-04-15 07:59:41,591 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1036] 2021-04-15 07:59:41,591 >>   Total optimization steps = 1130\n",
            "[INFO|trainer.py:1055] 2021-04-15 07:59:41,594 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1056] 2021-04-15 07:59:41,594 >>   Continuing training from epoch 8\n",
            "[INFO|trainer.py:1057] 2021-04-15 07:59:41,594 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1060] 2021-04-15 07:59:41,594 >>   Will skip the first 8 epochs then the first 96 batches in the first epoch.\n",
            "100% 1130/1130 [01:57<00:00,  1.47it/s][INFO|trainer.py:1217] 2021-04-15 08:01:38,660 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 117.0687, 'train_samples_per_second': 9.652, 'epoch': 10.0}\n",
            "100% 1130/1130 [01:57<00:00,  9.65it/s]\n",
            "[INFO|trainer.py:1686] 2021-04-15 08:01:39,193 >> Saving model checkpoint to /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out\n",
            "[INFO|configuration_utils.py:329] 2021-04-15 08:01:39,200 >> Configuration saved in /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-15 08:01:40,516 >> Model weights saved in /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-15 08:01:40,521 >> tokenizer config file saved in /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1913] 2021-04-15 08:01:40,524 >> Special tokens file saved in /content/gdrive/MyDrive/指向情緒案/data/annot_data/annotated_data_bkup/20210412/bert_ner/out/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:812] 2021-04-15 08:01:40,550 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,550 >>   epoch                      =       10.0\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,550 >>   init_mem_cpu_alloc_delta   =     1648MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,550 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,550 >>   init_mem_gpu_alloc_delta   =      388MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,550 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_mem_cpu_alloc_delta  =      417MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_mem_gpu_alloc_delta  =     1556MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_mem_gpu_peaked_delta =    12394MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_runtime              = 0:01:57.06\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_samples              =       1794\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:40,551 >>   train_samples_per_second   =      9.652\n",
            "04/15/2021 08:01:40 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1913] 2021-04-15 08:01:40,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1914] 2021-04-15 08:01:40,665 >>   Num examples = 448\n",
            "[INFO|trainer.py:1915] 2021-04-15 08:01:40,665 >>   Batch size = 8\n",
            "100% 56/56 [00:08<00:00,  6.39it/s]\n",
            "[INFO|trainer_pt_utils.py:812] 2021-04-15 08:01:49,707 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,707 >>   epoch                     =       10.0\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,707 >>   eval_A_f1                 =     0.3571\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,707 >>   eval_A_number             =         10\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,707 >>   eval_A_precision          =     0.2778\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,707 >>   eval_A_recall             =        0.5\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_C_f1                 =     0.3958\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_C_number             =         64\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_C_precision          =     0.5938\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_C_recall             =     0.2969\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_E_f1                 =     0.7048\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_E_number             =         41\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_E_precision          =     0.5781\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_E_recall             =     0.9024\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_O_f1                 =     0.8985\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_O_number             =        321\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_O_precision          =     0.8875\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_O_recall             =     0.9097\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_V_f1                 =     0.4706\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_V_number             =         12\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_V_precision          =        0.8\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_V_recall             =     0.3333\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_loss                 =     1.2611\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_mem_cpu_alloc_delta  =        4MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,708 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_mem_gpu_peaked_delta =      286MB\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_overall_accuracy     =     0.7969\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_overall_f1           =     0.7969\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_overall_precision    =     0.7969\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_overall_recall       =     0.7969\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_runtime              = 0:00:08.91\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_samples              =        448\n",
            "[INFO|trainer_pt_utils.py:817] 2021-04-15 08:01:49,709 >>   eval_samples_per_second   =     50.237\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}